{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import  models\n",
    "from visdom import Visdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义数据集\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.questionnaire_data = dataframe\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.questionnaire_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        questionnaire = self.questionnaire_data.iloc[idx, 2:].tolist()\n",
    "        label = self.questionnaire_data.iloc[idx, 1]\n",
    "        sample = {'questionnaire': questionnaire, 'label': label}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        return {'questionnaire': torch.Tensor(sample['questionnaire']), 'label':sample['label']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"quest_only.csv\", index_col=0)\n",
    "train_df = data_df.sample(frac=0.8,random_state=0,axis=0)\n",
    "test_df = data_df[~data_df.index.isin(train_df.index)]\n",
    "train_dataset = MyDataset(train_df,transform=ToTensor())\n",
    "test_dataset = MyDataset(test_df,transform=ToTensor())\n",
    "train_dataloader =DataLoader(train_dataset, batch_size=10,shuffle=True, num_workers=0)\n",
    "test_dataloader =DataLoader(test_dataset, batch_size=10,shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointNet(nn.Module):\n",
    "    def __init__(self,feature_extract=True, num_classes=3, hidden1=2048, hidden2=512, dropout=0.3):\n",
    "        super(JointNet, self).__init__()\n",
    "        model = models.vgg16(pretrained=True)\n",
    "        self.features = model.features\n",
    "        set_parameter_requires_grad(self.features, feature_extract)#固定特征提取层参数\n",
    "        self.avgpool=model.avgpool\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Linear(512*7*7 , hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden1 , hidden2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden2+37, hidden2+37),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden2+37, num_classes)\n",
    "        )\n",
    "        self.questonly = nn.Sequential(\n",
    "            nn.Linear(37, 64),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.ReLU(),            \n",
    "            nn.Linear(64, 64),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out=self.questonly(x)\n",
    "        return out\n",
    "    \n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=JointNet(feature_extract=False).to(device)\n",
    "learning_rate=0.001\n",
    "num_epochs = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'quest_acc'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viz = Visdom()\n",
    "viz.line([0.], [0], win='quest_train_loss', opts=dict(title='train_loss'))\n",
    "viz.line([[0.,0.]], [0], win='quest_acc', opts=dict(title='new_acc', legend=['train', 'test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0: loss0.96565, train_acc0.52239, test_acc0.35294\n",
      "epoch1: loss0.97873, train_acc0.56716, test_acc0.35294\n",
      "epoch2: loss0.81097, train_acc0.53731, test_acc0.35294\n",
      "epoch3: loss0.59787, train_acc0.55224, test_acc0.35294\n",
      "epoch4: loss1.31417, train_acc0.55224, test_acc0.35294\n",
      "epoch5: loss0.65297, train_acc0.58209, test_acc0.35294\n",
      "epoch6: loss0.88805, train_acc0.56716, test_acc0.35294\n",
      "epoch7: loss1.15435, train_acc0.62687, test_acc0.41176\n",
      "epoch8: loss1.12553, train_acc0.65672, test_acc0.52941\n",
      "epoch9: loss0.68451, train_acc0.73134, test_acc0.52941\n",
      "epoch10: loss1.05699, train_acc0.71642, test_acc0.52941\n",
      "epoch11: loss0.56246, train_acc0.71642, test_acc0.52941\n",
      "epoch12: loss0.68973, train_acc0.74627, test_acc0.52941\n",
      "epoch13: loss0.53882, train_acc0.73134, test_acc0.52941\n",
      "epoch14: loss1.12973, train_acc0.79104, test_acc0.52941\n",
      "epoch15: loss0.48136, train_acc0.77612, test_acc0.52941\n",
      "epoch16: loss0.51289, train_acc0.79104, test_acc0.52941\n",
      "epoch17: loss0.59152, train_acc0.79104, test_acc0.52941\n",
      "epoch18: loss0.97045, train_acc0.80597, test_acc0.52941\n",
      "epoch19: loss0.89752, train_acc0.76119, test_acc0.52941\n",
      "epoch20: loss0.34393, train_acc0.80597, test_acc0.52941\n",
      "epoch21: loss0.59496, train_acc0.80597, test_acc0.52941\n",
      "epoch22: loss0.17226, train_acc0.79104, test_acc0.52941\n",
      "epoch23: loss0.89719, train_acc0.82090, test_acc0.52941\n",
      "epoch24: loss0.50556, train_acc0.82090, test_acc0.52941\n",
      "epoch25: loss0.29123, train_acc0.80597, test_acc0.52941\n",
      "epoch26: loss0.61733, train_acc0.83582, test_acc0.52941\n",
      "epoch27: loss0.50329, train_acc0.79104, test_acc0.52941\n",
      "epoch28: loss0.65896, train_acc0.82090, test_acc0.52941\n",
      "epoch29: loss0.35541, train_acc0.83582, test_acc0.58824\n",
      "epoch30: loss0.35309, train_acc0.85075, test_acc0.52941\n",
      "epoch31: loss0.41735, train_acc0.83582, test_acc0.52941\n",
      "epoch32: loss0.36231, train_acc0.85075, test_acc0.52941\n",
      "epoch33: loss0.42445, train_acc0.83582, test_acc0.52941\n",
      "epoch34: loss0.59155, train_acc0.82090, test_acc0.52941\n",
      "epoch35: loss0.33789, train_acc0.83582, test_acc0.52941\n",
      "epoch36: loss0.40331, train_acc0.86567, test_acc0.52941\n",
      "epoch37: loss0.12526, train_acc0.83582, test_acc0.52941\n",
      "epoch38: loss1.03857, train_acc0.91045, test_acc0.52941\n",
      "epoch39: loss0.24521, train_acc0.86567, test_acc0.52941\n",
      "epoch40: loss0.31224, train_acc0.86567, test_acc0.52941\n",
      "epoch41: loss0.10162, train_acc0.89552, test_acc0.52941\n",
      "epoch42: loss0.15395, train_acc0.92537, test_acc0.52941\n",
      "epoch43: loss0.89396, train_acc0.86567, test_acc0.52941\n",
      "epoch44: loss0.48931, train_acc0.86567, test_acc0.52941\n",
      "epoch45: loss0.08698, train_acc0.88060, test_acc0.52941\n",
      "epoch46: loss0.28214, train_acc0.92537, test_acc0.52941\n",
      "epoch47: loss0.91706, train_acc0.94030, test_acc0.52941\n",
      "epoch48: loss0.41465, train_acc0.86567, test_acc0.52941\n",
      "epoch49: loss0.11822, train_acc0.89552, test_acc0.52941\n",
      "epoch50: loss0.54973, train_acc0.88060, test_acc0.52941\n",
      "epoch51: loss0.58353, train_acc0.91045, test_acc0.52941\n",
      "epoch52: loss0.09477, train_acc0.94030, test_acc0.52941\n",
      "epoch53: loss0.34565, train_acc0.91045, test_acc0.52941\n",
      "epoch54: loss0.09518, train_acc0.95522, test_acc0.52941\n",
      "epoch55: loss0.32442, train_acc0.91045, test_acc0.52941\n",
      "epoch56: loss0.14019, train_acc0.94030, test_acc0.52941\n",
      "epoch57: loss0.07037, train_acc0.91045, test_acc0.52941\n",
      "epoch58: loss0.30359, train_acc0.94030, test_acc0.52941\n",
      "epoch59: loss0.09802, train_acc0.94030, test_acc0.52941\n",
      "epoch60: loss0.17185, train_acc0.97015, test_acc0.52941\n",
      "epoch61: loss0.16117, train_acc0.94030, test_acc0.52941\n",
      "epoch62: loss0.10266, train_acc0.97015, test_acc0.52941\n",
      "epoch63: loss0.33501, train_acc0.97015, test_acc0.52941\n",
      "epoch64: loss0.09763, train_acc0.94030, test_acc0.52941\n",
      "epoch65: loss0.18011, train_acc0.97015, test_acc0.52941\n",
      "epoch66: loss0.08819, train_acc0.95522, test_acc0.52941\n",
      "epoch67: loss0.07605, train_acc0.92537, test_acc0.52941\n",
      "epoch68: loss0.03006, train_acc0.95522, test_acc0.52941\n",
      "epoch69: loss0.25211, train_acc0.95522, test_acc0.52941\n",
      "epoch70: loss0.04693, train_acc0.95522, test_acc0.52941\n",
      "epoch71: loss0.05847, train_acc0.95522, test_acc0.52941\n",
      "epoch72: loss0.12464, train_acc0.97015, test_acc0.52941\n",
      "epoch73: loss0.16142, train_acc0.94030, test_acc0.52941\n",
      "epoch74: loss0.22057, train_acc0.92537, test_acc0.52941\n",
      "epoch75: loss0.06072, train_acc0.97015, test_acc0.52941\n",
      "epoch76: loss0.04870, train_acc0.97015, test_acc0.52941\n",
      "epoch77: loss0.09165, train_acc0.97015, test_acc0.52941\n",
      "epoch78: loss0.20956, train_acc0.95522, test_acc0.52941\n",
      "epoch79: loss0.10076, train_acc0.94030, test_acc0.52941\n",
      "epoch80: loss0.06467, train_acc0.92537, test_acc0.52941\n",
      "epoch81: loss0.07565, train_acc1.00000, test_acc0.52941\n",
      "epoch82: loss0.15521, train_acc0.95522, test_acc0.52941\n",
      "epoch83: loss0.09773, train_acc0.97015, test_acc0.52941\n",
      "epoch84: loss0.24659, train_acc0.95522, test_acc0.52941\n",
      "epoch85: loss0.03119, train_acc0.97015, test_acc0.52941\n",
      "epoch86: loss0.17972, train_acc0.97015, test_acc0.52941\n",
      "epoch87: loss0.07990, train_acc0.98507, test_acc0.52941\n",
      "epoch88: loss0.18537, train_acc0.95522, test_acc0.52941\n",
      "epoch89: loss0.12204, train_acc1.00000, test_acc0.52941\n",
      "epoch90: loss0.04487, train_acc0.92537, test_acc0.52941\n",
      "epoch91: loss0.19467, train_acc0.97015, test_acc0.52941\n",
      "epoch92: loss0.07295, train_acc0.98507, test_acc0.52941\n",
      "epoch93: loss0.21533, train_acc0.97015, test_acc0.52941\n",
      "epoch94: loss0.07390, train_acc0.97015, test_acc0.52941\n",
      "epoch95: loss0.10242, train_acc0.98507, test_acc0.52941\n",
      "epoch96: loss0.07385, train_acc1.00000, test_acc0.52941\n",
      "epoch97: loss0.07495, train_acc0.98507, test_acc0.52941\n",
      "epoch98: loss0.01397, train_acc0.98507, test_acc0.52941\n",
      "epoch99: loss0.19489, train_acc0.98507, test_acc0.52941\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    for sample_batched in train_dataloader:\n",
    "        data = sample_batched['questionnaire'].to(device)\n",
    "        labels = sample_batched['label'].to(device)\n",
    "    \n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sample_batched in test_dataloader:\n",
    "            data = sample_batched['questionnaire'].to(device)\n",
    "            labels = sample_batched['label'].to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            \n",
    "    print(\"epoch%d: loss%.5f, train_acc%.5f, test_acc%.5f\" % (epoch, loss.item(),train_correct/train_total, test_correct/test_total))\n",
    "    viz.line([loss.item()], [epoch], win='quest_train_loss', update='append')\n",
    "    viz.line([[train_correct/train_total,test_correct/test_total]], [epoch], win='quest_acc', update='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"questmodel.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6413956a40eef355478ed87a1dcd368543de9fb360f14f6e88f24d01d865be95"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
