{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import  models\n",
    "from visdom import Visdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义数据集\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, transform=None):\n",
    "        self.questionnaire_data = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.questionnaire_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir,\"patient\"+str(self.questionnaire_data.iloc[idx, 0]),self.questionnaire_data.iloc[idx, 1])\n",
    "        image = io.imread(img_name)\n",
    "        questionnaire = self.questionnaire_data.iloc[idx, 3:].tolist()\n",
    "        label = self.questionnaire_data.iloc[idx, 2]\n",
    "        sample = {'image': image, 'questionnaire': questionnaire, 'label': label}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义Transforms变换\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        image = sample['image']\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1)).copy()\n",
    "        return {'image': torch.from_numpy(image).float(), 'questionnaire': torch.Tensor(sample['questionnaire']), 'label':sample['label']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"processed_data.csv\", index_col=0)\n",
    "train_df = data_df.sample(frac=0.8,random_state=0,axis=0)\n",
    "test_df = data_df[~data_df.index.isin(train_df.index)]\n",
    "train_dataset = MyDataset(train_df,\"patient\",transform=ToTensor())\n",
    "test_dataset = MyDataset(test_df,\"patient\",transform=ToTensor())\n",
    "train_dataloader =DataLoader(train_dataset, batch_size=1,shuffle=True, num_workers=0)\n",
    "test_dataloader =DataLoader(test_dataset, batch_size=10,shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointNet(nn.Module):\n",
    "    def __init__(self,feature_extract=True, num_classes=3, hidden1=2048, hidden2=512, dropout=0.3):\n",
    "        super(JointNet, self).__init__()\n",
    "        model = models.vgg16(pretrained=True)\n",
    "        self.features = model.features\n",
    "        set_parameter_requires_grad(self.features, feature_extract)#固定特征提取层参数\n",
    "        self.avgpool=model.avgpool\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Linear(512*7*7 , hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden1 , hidden2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden2+37, hidden2+37),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden2+37, num_classes)\n",
    "        )\n",
    "        self.classifier_onlyimg = nn.Sequential(\n",
    "            nn.Linear(hidden2, hidden2),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden2, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        img, quest = x\n",
    "        img = self.features(img)\n",
    "        img = self.avgpool(img)\n",
    "        img = img.view(img.size(0), 512*7*7)\n",
    "        img = self.hidden(img)\n",
    "        # joint = torch.cat([img, quest],1)\n",
    "        out=self.classifier_onlyimg(img)\n",
    "        return out\n",
    "    \n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=JointNet(feature_extract=False).to(device)\n",
    "learning_rate=0.001\n",
    "num_epochs = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'img_acc'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viz = Visdom()\n",
    "viz.line([0.], [0], win='img_train_loss', opts=dict(title='train_loss'))\n",
    "viz.line([[0.,0.]], [0], win='img_acc', opts=dict(title='new_acc', legend=['train', 'test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0: loss1.78514, train_acc0.63122, test_acc0.69091\n",
      "epoch1: loss1.85830, train_acc0.64706, test_acc0.69091\n",
      "epoch2: loss0.42385, train_acc0.64706, test_acc0.69091\n",
      "epoch3: loss0.31721, train_acc0.64706, test_acc0.69091\n",
      "epoch4: loss2.00429, train_acc0.64706, test_acc0.69091\n",
      "epoch5: loss0.48949, train_acc0.64706, test_acc0.69091\n",
      "epoch6: loss0.44784, train_acc0.64706, test_acc0.69091\n",
      "epoch7: loss1.89516, train_acc0.64706, test_acc0.69091\n",
      "epoch8: loss1.91263, train_acc0.64706, test_acc0.69091\n",
      "epoch9: loss0.45810, train_acc0.64706, test_acc0.69091\n",
      "epoch10: loss0.45750, train_acc0.64706, test_acc0.69091\n",
      "epoch11: loss1.64228, train_acc0.64706, test_acc0.69091\n",
      "epoch12: loss0.40568, train_acc0.64706, test_acc0.69091\n",
      "epoch13: loss1.81916, train_acc0.64706, test_acc0.69091\n",
      "epoch14: loss0.43727, train_acc0.64706, test_acc0.69091\n",
      "epoch15: loss0.46483, train_acc0.64706, test_acc0.69091\n",
      "epoch16: loss0.43315, train_acc0.64706, test_acc0.69091\n",
      "epoch17: loss0.43513, train_acc0.64706, test_acc0.69091\n",
      "epoch18: loss0.53707, train_acc0.64706, test_acc0.69091\n",
      "epoch19: loss0.37688, train_acc0.64706, test_acc0.69091\n",
      "epoch20: loss0.42179, train_acc0.64706, test_acc0.69091\n",
      "epoch21: loss0.40221, train_acc0.64706, test_acc0.69091\n",
      "epoch22: loss0.46136, train_acc0.64706, test_acc0.69091\n",
      "epoch23: loss1.68556, train_acc0.64706, test_acc0.69091\n",
      "epoch24: loss0.43676, train_acc0.64706, test_acc0.69091\n",
      "epoch25: loss1.63164, train_acc0.64706, test_acc0.69091\n",
      "epoch26: loss0.40643, train_acc0.64706, test_acc0.69091\n",
      "epoch27: loss0.36718, train_acc0.64706, test_acc0.69091\n",
      "epoch28: loss1.73180, train_acc0.64706, test_acc0.69091\n",
      "epoch29: loss0.44061, train_acc0.64706, test_acc0.69091\n",
      "epoch30: loss0.45264, train_acc0.64706, test_acc0.69091\n",
      "epoch31: loss1.78813, train_acc0.64706, test_acc0.69091\n",
      "epoch32: loss1.64711, train_acc0.64706, test_acc0.69091\n",
      "epoch33: loss0.47005, train_acc0.64706, test_acc0.69091\n",
      "epoch34: loss1.78204, train_acc0.64706, test_acc0.69091\n",
      "epoch35: loss0.43575, train_acc0.64706, test_acc0.69091\n",
      "epoch36: loss0.46379, train_acc0.64706, test_acc0.69091\n",
      "epoch37: loss1.70982, train_acc0.64706, test_acc0.69091\n",
      "epoch38: loss1.70796, train_acc0.64706, test_acc0.69091\n",
      "epoch39: loss0.40175, train_acc0.64706, test_acc0.69091\n",
      "epoch40: loss0.40029, train_acc0.64706, test_acc0.69091\n",
      "epoch41: loss0.41847, train_acc0.64706, test_acc0.69091\n",
      "epoch42: loss0.50424, train_acc0.64706, test_acc0.69091\n",
      "epoch43: loss0.40886, train_acc0.64706, test_acc0.69091\n",
      "epoch44: loss0.46562, train_acc0.64706, test_acc0.69091\n",
      "epoch45: loss1.76161, train_acc0.64706, test_acc0.69091\n",
      "epoch46: loss1.93305, train_acc0.64706, test_acc0.69091\n",
      "epoch47: loss0.41659, train_acc0.64706, test_acc0.69091\n",
      "epoch48: loss1.72205, train_acc0.64706, test_acc0.69091\n",
      "epoch49: loss1.71135, train_acc0.64706, test_acc0.69091\n",
      "epoch50: loss0.39505, train_acc0.64706, test_acc0.69091\n",
      "epoch51: loss0.42145, train_acc0.64706, test_acc0.69091\n",
      "epoch52: loss0.47084, train_acc0.64706, test_acc0.69091\n",
      "epoch53: loss0.46789, train_acc0.64706, test_acc0.69091\n",
      "epoch54: loss1.54029, train_acc0.64706, test_acc0.69091\n",
      "epoch55: loss1.78252, train_acc0.64706, test_acc0.69091\n",
      "epoch56: loss0.47704, train_acc0.64706, test_acc0.69091\n",
      "epoch57: loss0.39902, train_acc0.64706, test_acc0.69091\n",
      "epoch58: loss0.43734, train_acc0.64706, test_acc0.69091\n",
      "epoch59: loss0.40949, train_acc0.64706, test_acc0.69091\n",
      "epoch60: loss1.85238, train_acc0.64706, test_acc0.69091\n",
      "epoch61: loss0.48425, train_acc0.64706, test_acc0.69091\n",
      "epoch62: loss0.46619, train_acc0.64706, test_acc0.69091\n",
      "epoch63: loss0.49218, train_acc0.64706, test_acc0.69091\n",
      "epoch64: loss0.45029, train_acc0.64706, test_acc0.69091\n",
      "epoch65: loss0.37255, train_acc0.64706, test_acc0.69091\n",
      "epoch66: loss0.42711, train_acc0.64706, test_acc0.69091\n",
      "epoch67: loss0.43755, train_acc0.64706, test_acc0.69091\n",
      "epoch68: loss0.51252, train_acc0.64706, test_acc0.69091\n",
      "epoch69: loss1.55456, train_acc0.64706, test_acc0.69091\n",
      "epoch70: loss0.43847, train_acc0.64706, test_acc0.69091\n",
      "epoch71: loss0.48938, train_acc0.64706, test_acc0.69091\n",
      "epoch72: loss0.37723, train_acc0.64706, test_acc0.69091\n",
      "epoch73: loss0.46679, train_acc0.64706, test_acc0.69091\n",
      "epoch74: loss0.43249, train_acc0.64706, test_acc0.69091\n",
      "epoch75: loss1.68366, train_acc0.64706, test_acc0.69091\n",
      "epoch76: loss0.40452, train_acc0.64706, test_acc0.69091\n",
      "epoch77: loss0.42338, train_acc0.64706, test_acc0.69091\n",
      "epoch78: loss0.43838, train_acc0.64706, test_acc0.69091\n",
      "epoch79: loss1.88423, train_acc0.64706, test_acc0.69091\n",
      "epoch80: loss0.46333, train_acc0.64706, test_acc0.69091\n",
      "epoch81: loss1.77489, train_acc0.64706, test_acc0.69091\n",
      "epoch82: loss0.46697, train_acc0.64706, test_acc0.69091\n",
      "epoch83: loss0.52219, train_acc0.64706, test_acc0.69091\n",
      "epoch84: loss0.46470, train_acc0.64706, test_acc0.69091\n",
      "epoch85: loss0.42720, train_acc0.64706, test_acc0.69091\n",
      "epoch86: loss0.42922, train_acc0.64706, test_acc0.69091\n",
      "epoch87: loss0.45110, train_acc0.64706, test_acc0.69091\n",
      "epoch88: loss0.40478, train_acc0.64706, test_acc0.69091\n",
      "epoch89: loss1.89357, train_acc0.64706, test_acc0.69091\n",
      "epoch90: loss0.37592, train_acc0.64706, test_acc0.69091\n",
      "epoch91: loss0.47437, train_acc0.64706, test_acc0.69091\n",
      "epoch92: loss0.40477, train_acc0.64706, test_acc0.69091\n",
      "epoch93: loss0.39746, train_acc0.64706, test_acc0.69091\n",
      "epoch94: loss1.80798, train_acc0.64706, test_acc0.69091\n",
      "epoch95: loss0.52636, train_acc0.64706, test_acc0.69091\n",
      "epoch96: loss1.70986, train_acc0.64706, test_acc0.69091\n",
      "epoch97: loss0.51074, train_acc0.64706, test_acc0.69091\n",
      "epoch98: loss0.42348, train_acc0.64706, test_acc0.69091\n",
      "epoch99: loss0.44804, train_acc0.64706, test_acc0.69091\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    for sample_batched in train_dataloader:\n",
    "        data = [sample_batched['image'].to(device),sample_batched['questionnaire'].to(device)]\n",
    "        labels = sample_batched['label'].to(device)\n",
    "    \n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sample_batched in test_dataloader:\n",
    "            data = [sample_batched['image'].to(device),sample_batched['questionnaire'].to(device)]\n",
    "            labels = sample_batched['label'].to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            \n",
    "    print(\"epoch%d: loss%.5f, train_acc%.5f, test_acc%.5f\" % (epoch, loss.item(),train_correct/train_total, test_correct/test_total))\n",
    "    viz.line([loss.item()], [epoch], win='img_train_loss', update='append')\n",
    "    viz.line([[train_correct/train_total,test_correct/test_total]], [epoch], win='img_acc', update='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"imgmodel.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6413956a40eef355478ed87a1dcd368543de9fb360f14f6e88f24d01d865be95"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
